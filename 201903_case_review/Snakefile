import gzip
import csv
from collections import namedtuple
from pathlib import Path

OMIC_UUID_MANIFEST = '../201901_locate_discovery_data/tracked_results/CPTAC3_GBM_GDC_omics_UUIDs.tsv'
KATMAI_MAP = '../matt_catalog/katmai.BamMap.dat'
GDC_REF_FA_PTH = '/diskmnt/Datasets/Reference/GRCh38.d1.vd1/GRCh38.d1.vd1.fa'
WXS_TARGET_ROI_BED_PTH = '../201901_locate_discovery_data/annotations/hg38_exome_targeted_regions.bed.gz'
GATK_DOCKER_IMG = 'broadinstitute/gatk:4.1.0.0'
# Path to the folder of Snakefile inside the Docker image
PROJ_FOLDER_IN_DOCKER = '/repo/201903_case_review'

# Map GDC seq data to local paths {{{
# Create a UUID to local file path map
uuid_to_local_file_pth = {}
with open(KATMAI_MAP) as f:
    reader = csv.DictReader(f, dialect='excel-tab')
    for row in reader:
        # Keep only WGS/WXS UUIDs
        if row['experimental_strategy'] in ('WGS', 'WXS'):
            uuid_to_local_file_pth[row['UUID']] = Path(row['data_path'])


# Link the WGS/WXS BAMs and their indices
sample_to_wgs_bam_pth = {}
sample_to_wxs_bam_pth = {}
with open(OMIC_UUID_MANIFEST) as f:
    reader = csv.DictReader(f, dialect='excel-tab')
    for row in reader:
        sample = row['case']
        sample_to_wgs_bam_pth[sample] = {}
        sample_to_wxs_bam_pth[sample] = {}

        # Link WGS BAMs
        normal_bam_uuid = row['wgs_blood_normal_bam']
        tumor_bam_uuid = row['wgs_tumor_bam']

        for bam_type, uuid in [
            ('normal', normal_bam_uuid),
            ('tumor', tumor_bam_uuid),
        ]:
            if uuid == 'NA':
                continue
            sample_to_wgs_bam_pth[sample][bam_type] = uuid_to_local_file_pth[uuid]

        # Link WXS BAMs
        normal_bam_uuid = row['wxs_blood_normal_bam']
        tumor_bam_uuid = row['wxs_tumor_bam']

        for bam_type, uuid in [
            ('normal', normal_bam_uuid),
            ('tumor', tumor_bam_uuid),
        ]:
            if uuid == 'NA':
                continue
            sample_to_wxs_bam_pth[sample][bam_type] = uuid_to_local_file_pth[uuid]

# }}}


rule link_gdc_bam:
    """Link one WGS BAM locally."""
    input: manifest=OMIC_UUID_MANIFEST,
           local_map=KATMAI_MAP
    output:
        bam='external_data/GDC_{seq}_bam/{sample}.{bam_type}.bam',
        bai='external_data/GDC_{seq}_bam/{sample}.{bam_type}.bam.bai'
    run:
        if wildcards.seq == 'WGS':
            bam_map = sample_to_wgs_bam_pth
        elif wildcards.seq == 'WXS':
            bam_map = sample_to_wxs_bam_pth
        else:
            raise ValueError(f'Unknown sequencing type {wildcards.seq}')
        bam_src_pth = bam_map[wildcards.sample][wildcards.bam_type]
        bai_src_pth = bam_src_pth.with_suffix('.bam.bai')

        bam_dst_pth = Path(output['bam'])
        bai_dst_pth = Path(output['bai'])
        bam_dst_pth.symlink_to(bam_src_pth)
        bai_dst_pth.symlink_to(bai_src_pth)


# Calculate the read depth {{{
def cal_additional_threads(wildcards, input, output, threads):
    return threads - 1


rule calculate_wgs_bam_depth:
    """Calculate the WGS BAM sequencing depth."""
    output: 'processed_data/wgs_depth/{sample}.{bam_type}.regions.bed.gz'
    input:
        bam=rules.link_gdc_bam.output['bam'].replace('{seq}', 'WGS')
    threads: 3
    params:
        add_threads=cal_additional_threads
    shell:
        "mosdepth "
        "-n --fast-mode "
        "--threads {params.add_threads} "
        "--by 500 "
        "processed_data/wgs_depth/{wildcards.sample}.{wildcards.bam_type} "
        "{input}"


rule calculate_wxs_bam_depth:
    """Calculate the WXS BAM sequencing depth."""
    output: 'processed_data/wxs_depth/{sample}.{bam_type}.regions.bed.gz'
    input:
        bam=rules.link_gdc_bam.output['bam'].replace('{seq}', 'WXS'),
        wxs_roi=WXS_TARGET_ROI_BED_PTH
    threads: 3
    params:
        add_threads=cal_additional_threads
    shell:
        "mosdepth "
        "-n --fast-mode "
        "--threads {params.add_threads} "
        "--by {input.wxs_roi} "
        "processed_data/wxs_depth/{wildcards.sample}.{wildcards.bam_type} "
        "{input.bam}"


wxs_bams_of_interest = [
    ('C3L-01834', 'normal'),
    ('C3L-01834', 'tumor')
]

rule calc_bam_depths_of_interest:
    """Caculate the WXS BAM depth of of interest."""
    input: [rules.calculate_wxs_bam_depth.output[0].format(sample=sample, bam_type=bam_type) \
            for sample, bam_type in wxs_bams_of_interest]
# }}}


# GATK4 somatic WGS CNV (PoN) {{{

# Use all GBM normals in the discovery cohort as PoN
PON_SAMPLES = set()
TUMOR_SAMPLES = set()
for sample, bam_map in sample_to_wgs_bam_pth.items():
    if 'normal' in bam_map:
        PON_SAMPLES.add(sample)
    if 'tumor' in bam_map:
        TUMOR_SAMPLES.add(sample)


rule gatk_preprocessintervals:
    """GATK PreprocessIntervals for WGS."""
    input: ref_fa=GDC_REF_FA_PTH,
    output: 'processed_data/gatk4_somatic_cnv/interval_lists/preprocessed_intervals.interval_list'
    log: 'logs/gatk_cnv/gatk_preprocessedintervals.log'
    shell:
        """
        docker run -t --rm -u $(id -u):$(id -g)    \
            -v /diskmnt:/diskmnt                   \
            -v $(dirname $PWD):/repo               \
            {GATK_DOCKER_IMG}                      \
            gatk PreprocessIntervals \
                -R {input.ref_fa} \
                --bin-length 1000 \
                --padding 0 \
                -interval-merging-rule OVERLAPPING_ONLY \
                -O {PROJ_FOLDER_IN_DOCKER}/{output} \
            2>{log} 1>&2
        """


rule filter_gatk_cnv_intervals:
    """Filter the CNV intervals to include only the canonoical autosomal chromosomes."""
    input: rules.gatk_preprocessintervals.output[0]
    output: 'processed_data/gatk4_somatic_cnv/interval_lists/preprocessed_intervals.canonical_autosomal_chroms.interval_list'
    run:
        # Take only chr1, ..., chr22
        accepted_chroms = set(f'chr{i}' for i in range(1, 23))
        with open(input[0]) as f, open(output[0], 'w') as outf:
            for line in f:
                # Write out all the headers
                if line.startswith('@'):
                    outf.write(line)
                else:
                    chrom = line.split('\t', 1)[0]
                    if chrom in accepted_chroms:
                        outf.write(line)


rule gatk_collectreadcounts:
    """Run GATK CollectReadCounts on one BAM."""
    output:
        hdf5='processed_data/gatk4_somatic_cnv/readcounts_hdf5/{bam_type}/{sample}.counts.hdf5'
    input:
        bam='external_data/GDC_WGS_bam/{sample}.{bam_type}.bam',
        bai='external_data/GDC_WGS_bam/{sample}.{bam_type}.bam.bai',
        interval_list=rules.filter_gatk_cnv_intervals.output[0]
    log: 'logs/gatk_cnv/collectreadcounts/{bam_type}/{sample}.log'
    shell:
        """
        docker run -t --rm -u $(id -u):$(id -g) \
            --group-add $(grep cptac /etc/group | cut -d: -f3) \
            -v /diskmnt:/diskmnt                \
            -v $(dirname $PWD):/repo            \
            {GATK_DOCKER_IMG}                   \
            gatk CollectReadCounts \
                -I {PROJ_FOLDER_IN_DOCKER}/{input.bam} \
                -L {PROJ_FOLDER_IN_DOCKER}/{input.interval_list} \
                --interval-merging-rule OVERLAPPING_ONLY \
                -O {PROJ_FOLDER_IN_DOCKER}/{output.hdf5} \
            2>{log} 1>&2
        """


rule gatk_pon_readcounts:
    """Generate readcounts HDF5 of all PoN samples."""
    input: expand(rules.gatk_collectreadcounts.output['hdf5'], \
                  bam_type='normal', sample=PON_SAMPLES)


rule gatk_tumor_readcounts:
    """Generate readcounts HDF5 of all tumor samples."""
    input: expand(rules.gatk_collectreadcounts.output['hdf5'], \
                  bam_type='tumor', sample=TUMOR_SAMPLES)



rule gatk_annotateintervals:
    """Run GATK AnnotateIntervals."""
    input:
        ref_fa=GDC_REF_FA_PTH,
        interval_list=rules.filter_gatk_cnv_intervals.output[0]
    output: 'processed_data/gatk4_somatic_cnv/annotated_intervals/annotated_intervals.canonical_autosomal_chroms.tsv'
    log: 'logs/gatk_cnv/gatk_annotateintervals.log'
    shell:
        """
        docker run -t --rm -u $(id -u):$(id -g)    \
            -v /diskmnt:/diskmnt                   \
            -v $(dirname $PWD):/repo               \
            {GATK_DOCKER_IMG}                      \
            gatk AnnotateIntervals \
                -R {input.ref_fa} \
                -L {PROJ_FOLDER_IN_DOCKER}/{input.interval_list} \
                --interval-merging-rule OVERLAPPING_ONLY \
                -O {PROJ_FOLDER_IN_DOCKER}/{output} \
            2>{log} 1>&2
        """


rule gatk_createreadcountpon:
    """Run GATK CreateReadCountPanelOfNormals on all PoN normals."""
    output: 'processed_data/gatk4_somatic_cnv/cnv.pon.hdf5'
    input:
        pon_readcounts=rules.gatk_pon_readcounts.input,
        annotated_intervals=rules.gatk_annotateintervals.output[0]
    params:
        pon_pth_args=lambda wildcards, input: \
            [f'-I {PROJ_FOLDER_IN_DOCKER}/{pth}' for pth in input['pon_readcounts']]
    log: 'logs/gatk_cnv/create_pon_hdf5.log'
    shell:
        # A hack to make sure there is username and groupname of the mapped uid
        # and gid inside the Docker instance.
        #
        # my_password and my_group is created by appending the /etc/passwd and
        # /etc/group inside the Docker image with my username and cptac group
        # (with correct uid/gid mapping).
        """
        docker run -t --rm -u $(id -u):$(id -g)    \
            -v $PWD/my_passwd:/etc/passwd          \
            -v $PWD/my_group:/etc/group            \
            -v /diskmnt:/diskmnt                   \
            -v $(dirname $PWD):/repo               \
            {GATK_DOCKER_IMG}                      \
            gatk --java-options "-Xmx16000m" \
                CreateReadCountPanelOfNormals \
                {params.pon_pth_args} \
                --annotated-intervals {PROJ_FOLDER_IN_DOCKER}/{input.annotated_intervals} \
                -O {PROJ_FOLDER_IN_DOCKER}/{output} \
            2>{log} 1>&2
        """
# }}}
